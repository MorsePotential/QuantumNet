
@article{morbidi_distributed_2014,
	title = {A Distributed Solution to the Network Reconstruction Problem},
	volume = {70},
	url = {https://hal.archives-ouvertes.fr/hal-01001828},
	doi = {10.1016/j.sysconle.2014.05.008},
	abstract = {It has been recently shown in Ren et al. (2010) that by collecting noise-contaminated time series generated by a coupled-oscillator system at each node of a network, it is possible to robustly reconstruct its topology, i.e. determine the graph Laplacian. Restricting ourselves to linear consensus dynamics over undirected communication networks, in this paper we introduce a new dynamic average consensus least-squares algorithm to locally estimate these time series at each node, thus making the reconstruction process fully distributed and more easily applicable in the real world. We also propose a novel efficient method for separating the off-diagonal entries of the reconstructed Laplacian, and examine several concepts related to the trace of the dynamic correlation matrix of the coupled single integrators, which is a distinctive element of our network reconstruction method. The theory is illustrated with examples from computer, power and transportation systems.},
	pages = {85--91},
	journaltitle = {Systems and Control Letters},
	author = {Morbidi, Fabio and Kibangou, Alain Y.},
	urldate = {2020-05-29},
	date = {2014-06},
	note = {Publisher: Elsevier},
	keywords = {Consensus algorithms, Distributed estimation, Network reconstruction, Networked systems},
	file = {Morbidi_Kibangou_2014_A Distributed Solution to the Network Reconstruction Problem.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\6LG2DJHG\\Morbidi_Kibangou_2014_A Distributed Solution to the Network Reconstruction Problem.pdf:application/pdf}
}

@inproceedings{materassi_network_2012,
	location = {Maui, {HI}, {USA}},
	title = {Network reconstruction of dynamical polytrees with unobserved nodes},
	isbn = {978-1-4673-2066-5 978-1-4673-2065-8 978-1-4673-2063-4 978-1-4673-2064-1},
	url = {http://ieeexplore.ieee.org/document/6426335/},
	doi = {10.1109/CDC.2012.6426335},
	abstract = {The paper deals with the problem of unveiling the link structure of a network of linear dynamical systems. A technique is provided guaranteeing an exact detection of the links of a network of dynamical systems with no undirected cycles (Linear Dynamic Polytrees). In particular, the presence of unobserved (latent) nodes is taken into account. Knowledge on the speciﬁc number of hidden processes is not required. It is proven that the topology can be consistently reconstructed, as long the degree of each latent node is at least three with outdegree of at least two. The result extends previous work that was limited to a more restricted class of dynamical systems (Rooted Trees).},
	eventtitle = {2012 {IEEE} 51st Annual Conference on Decision and Control ({CDC})},
	pages = {4629--4634},
	booktitle = {2012 {IEEE} 51st {IEEE} Conference on Decision and Control ({CDC})},
	publisher = {{IEEE}},
	author = {Materassi, Donatello and Salapaka, Murti V.},
	urldate = {2020-05-29},
	date = {2012-12},
	langid = {english},
	file = {Materassi and Salapaka - 2012 - Network reconstruction of dynamical polytrees with.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\PC8L9PSS\\Materassi and Salapaka - 2012 - Network reconstruction of dynamical polytrees with.pdf:application/pdf}
}

@article{danilov_methods_2016,
	title = {Methods of graph network reconstruction in personalized medicine: {METHODS} {OF} {GRAPH} {NETWORK} {RECONSTRUCTION} {IN} {PERSONALIZED} {MEDICINE}},
	volume = {32},
	issn = {20407939},
	url = {http://doi.wiley.com/10.1002/cnm.2754},
	doi = {10.1002/cnm.2754},
	shorttitle = {Methods of graph network reconstruction in personalized medicine},
	abstract = {The paper addresses methods for generation of individualized computational domains on the basis of medical imaging dataset. The computational domains will be used in one-dimensional (1D) and three-dimensional (3D)–1D coupled hemodynamic models. A 1D hemodynamic model employs a 1D network of a patientspeciﬁc vascular network with large number of vessels. The 1D network is the graph with nodes in the 3D space which bears additional geometric data such as length and radius of vessels. A 3D hemodynamic model requires a detailed 3D reconstruction of local parts of the vascular network. We propose algorithms which extend the automated segmentation of vascular and tubular structures, generation of centerlines, 1D network reconstruction, correction, and local adaptation. We consider two modes of centerline representation: (i) skeletal segments or sets of connected voxels and (ii) curved paths with corresponding radii. Individualized reconstruction of 1D networks depends on the mode of centerline representation. Efﬁciency of the proposed algorithms is demonstrated on several examples of 1D network reconstruction. The networks can be used in modeling of blood ﬂows as well as other physiological processes in tubular structures. Copyright © 2015 John Wiley \& Sons, Ltd.},
	pages = {e02754},
	number = {8},
	journaltitle = {Int. J. Numer. Meth. Biomed. Engng.},
	author = {Danilov, A. and Ivanov, Yu. and Pryamonosov, R. and Vassilevski, Yu.},
	urldate = {2020-05-29},
	date = {2016-08},
	langid = {english},
	file = {Danilov et al. - 2016 - Methods of graph network reconstruction in persona.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\5FNEXGGP\\Danilov et al. - 2016 - Methods of graph network reconstruction in persona.pdf:application/pdf}
}

@article{braunstein_network_2018,
	title = {Network reconstruction from infection cascades},
	url = {http://arxiv.org/abs/1609.00432},
	abstract = {Accessing the network through which a propagation dynamics diffuse is essential for understanding and controlling it. In a few cases, such information is available through direct experiments or thanks to the very nature of propagation data. In a majority of cases however, available information about the network is indirect and comes from partial observations of the dynamics, rendering the network reconstruction a fundamental inverse problem. Here we show that it is possible to reconstruct the whole structure of an interaction network and to simultaneously infer the complete time course of activation spreading, relying just on single epoch (i.e. snapshot) or time-scattered observations of a small number of activity cascades. The method that we present is built on a Belief Propagation approximation, that has shown impressive accuracy in a wide variety of relevant cases, and is able to infer interactions in presence of incomplete time-series data by providing a detailed modeling of the posterior distribution of trajectories conditioned to the observations. Furthermore, we show by experiments that the information content of full cascades is relatively smaller than that of sparse observations or single snapshots.},
	journaltitle = {{arXiv}:1609.00432 [cond-mat, physics:physics, q-bio]},
	author = {Braunstein, Alfredo and Ingrosso, Alessandro and Muntoni, Anna Paola},
	urldate = {2020-05-29},
	date = {2018-02-12},
	eprinttype = {arxiv},
	eprint = {1609.00432},
	keywords = {Computer Science - Social and Information Networks, Condensed Matter - Disordered Systems and Neural Networks, Physics - Physics and Society, Quantitative Biology - Populations and Evolution},
	file = {Braunstein et al_2018_Network reconstruction from infection cascades.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\HV6HM2QH\\Braunstein et al_2018_Network reconstruction from infection cascades.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\IWHIEBA9\\1609.html:text/html}
}

@article{arendas_spectra_2016,
	title = {On spectra of spectra},
	volume = {54},
	url = {10.1007/s10910-016-0591-1},
	doi = {10.1007/s10910-016-0591-1},
	abstract = {Spectroscopic networks ({SNs}), where the vertices are discrete, rovibronic energy levels and the edges are transitions among the levels allowed by quantum mechanics, serve as useful models helping to understand high-resolution spectra of molecular systems. The experimental {SNs} of the (Formula presented.), and (Formula presented.) molecules, containing a single copy of the known measured and assigned transitions, are investigated via the corresponding network representation matrices, including the Ritz matrix (Formula presented.), the adjacency matrix (Formula presented.), the combinatorial Laplacian matrix (Formula presented.), and the normalized Laplacian matrix (Formula presented.). Using elements of graph (network) theory and the eigenvalue spectra of the matrices mentioned, several interesting results relevant for high-resolution molecular spectroscopy are revealed about the structure of the investigated {SNs}. For example, as long as the parity selection rule of molecular rovibrational transitions is not violated, the experimental {SNs} investigated not only contain, with the exception of (Formula presented.), two principal components but they are all bipartite networks, as proven by the symmetry of the eigenvalue spectrum of (Formula presented.) about the origin. Furthermore, the {PageRank} ordering system is introduced to molecular spectroscopy to identify the most important vertices of {SNs}. The rankings provided by the degree of the levels and by {PageRank} may differ significantly; it appears that {PageRank} provides the more useful ranking. The connectors of relatively dense clusters of the {SNs} are identified and analysed via spectral clustering techniques based on (Formula presented.) and (Formula presented.). The identification of connectors becomes especially important when judging the true accuracy of the experimental rovibrational energy levels obtained through the Measured Active Rotational-Vibrational Energy Levels ({MARVEL}) approach, built with the help of the Ritz matrix (Formula presented.). © 2016, Springer International Publishing Switzerland.},
	pages = {806--822},
	number = {3},
	journaltitle = {Journal of Mathematical Chemistry},
	author = {Árendás, P. and Furtenbacher, T. and Császár, A.G.},
	date = {2016},
	keywords = {Bipartiteness, Eigenvalues, Matrix representations, {PageRank}, Spectral clustering, Spectroscopic network},
	file = {SCOPUS Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\LKSN7GUB\\display.html:text/html}
}

@article{furtenbacher_role_2012,
	title = {The role of intensities in determining characteristics of spectroscopic networks},
	volume = {1009},
	url = {10.1016/j.molstruc.2011.10.057},
	doi = {10.1016/j.molstruc.2011.10.057},
	abstract = {Spectroscopic networks ({SNs}) are large, finite, weighted, undirected, rooted graphs, where the vertices are discrete energy levels, the edges are transitions, and the weights are transition intensities. While first-principles {SNs} are "deterministic" by definition, if a realistic transition intensity cut-off is employed during the construction of these {SNs}, a certain randomness ("stochasticity") is introduced. Experiments naturally build random graphs. It is shown on the example of the {HD} 16O isotopologue of the water molecule how intensities, in the present case one-photon absorption intensities, determine the structure as well as the degree distribution and edge density of {SNs}. The degree distribution of realistic computed {SNs} can be described as scale free, with the usual and well known consequences. Experimental {SNs}, based on measured and assigned transitions, also turn out to be scale free. The graph-theoretical view of high-resolution molecular spectra offers several new ideas for improving the accuracy and robustness of information systems containing spectroscopic data. For example, it is shown that most all rotational-vibrational energy levels are involved in at least a few relatively strong transitions suggesting that an almost complete coverage of experimental quality energy levels can be deduced from measurements. © 2011 Elsevier B.V. All rights reserved.},
	pages = {123--129},
	journaltitle = {Journal of Molecular Structure},
	author = {Furtenbacher, T. and Császár, A.G.},
	date = {2012},
	keywords = {Spectroscopic network, {HD} 16O, {MARVEL}, One-photon absorption spectra, Scale-free degree distribution},
	file = {SCOPUS Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\DPIC9LAH\\display.html:text/html}
}

@article{tobias_cycle_2017,
	title = {Cycle bases to the rescue},
	volume = {203},
	url = {10.1016/j.jqsrt.2017.03.031},
	doi = {10.1016/j.jqsrt.2017.03.031},
	abstract = {Cycle bases of graph theory are introduced for the analysis of transition data deposited in line-by-line rovibronic spectroscopic databases. The principal advantage of using cycle bases is that outlier transitions –almost always present in spectroscopic databases built from experimental data originating from many different sources– can be detected and identified straightforwardly and automatically. The data available for six water isotopologues, H2 16O, H2 17O, H2 18O, {HD}16O, {HD}17O, and {HD}18O, in the {HITRAN}2012 and {GEISA}2015 databases are used to demonstrate the utility of cycle-basis-based outlier-detection approaches. The spectroscopic databases appear to be sufficiently complete so that the great majority of the entries of the minimum cycle basis have the minimum possible length of four. More than 2000 transition conflicts have been identified for the isotopologue H2 16O in the {HITRAN}2012 database, the seven common conflict types are discussed. It is recommended to employ cycle bases, and especially a minimum cycle basis, for the analysis of transitions deposited in high-resolution spectroscopic databases. © 2017 Elsevier Ltd},
	pages = {557--564},
	journaltitle = {Journal of Quantitative Spectroscopy and Radiative Transfer},
	author = {Tóbiás, R. and Furtenbacher, T. and Császár, A.G.},
	date = {2017},
	keywords = {{MARVEL}, Cycle basis, Database, {ECART}, Energy levels, Information system, Rovibronic transitions},
	file = {SCOPUS Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\7NB8J3YK\\display.html:text/html}
}

@article{furtenbacher_simple_2014,
	title = {Simple molecules as complex systems},
	volume = {4},
	url = {10.1038/srep04654},
	doi = {10.1038/srep04654},
	abstract = {For individual molecules quantum mechanics ({QM}) offers a simple, natural and elegant way to build large-scale complex networks: quantized energy levels are the nodes, allowed transitions among the levels are the links, and transition intensities supply the weights. {QM} networks are intrinsic properties of molecules and they are characterized experimentally via spectroscopy; thus, realizations of {QM} networks are called spectroscopic networks ({SN}). As demonstrated for the rovibrational states of H216O, the molecule governing the greenhouse effect on earth through hundreds of millions of its spectroscopic transitions (links), both the measured and first-principles computed one-photon absorption {SNs} containing experimentally accessible transitions appear to have heavy-tailed degree distributions. The proposed novel view of high-resolution spectroscopy and the observed degree distributions have important implications: appearance of a core of highly interconnected hubs among the nodes, a generally disassortative connection preference, considerable robustness and error tolerance, and an â œ ultra-small-worldâ A property. The network-theoretical view of spectroscopy offers a data reduction facility via a minimum-weight spanning tree approach, which can assist high-resolution spectroscopists to improve the efficiency of the assignment of their measured spectra.},
	journaltitle = {Scientific Reports},
	author = {Furtenbacher, T. and Árendás, P. and Mellau, G. and Császár, A.G.},
	date = {2014},
	file = {SCOPUS Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\XQ5CCYPA\\display.html:text/html;Furtenbacher et al_2014_Simple molecules as complex systems.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\XGJARDF7\\Furtenbacher et al_2014_Simple molecules as complex systems.pdf:application/pdf}
}

@article{chubb_marvel_2018,
	title = {{MARVEL} analysis of the measured high-resolution rovibrational spectra of C2H2},
	volume = {204},
	issn = {0022-4073},
	url = {10.1016/j.jqsrt.2017.08.018},
	doi = {10.1016/j.jqsrt.2017.08.018},
	abstract = {Rotation-vibration energy levels are determined for the electronic ground state of the acetylene molecule, 12C2H2, using the Measured Active Rotational-Vibrational Energy Levels (Marvel) technique. 37,813 measured transitions from 61 publications are considered. The distinct components of the spectroscopic network linking ortho and para states of the molecule are considered separately. The 20,717 ortho and 17,096 para transitions measured experimentally are used to determine 6013 ortho and 5200 para energy levels. The Marvel results are compared with alternative compilations based on the use of effective Hamiltonians.},
	pages = {42--55},
	journaltitle = {Journal of Quantitative Spectroscopy and Radiative Transfer},
	author = {Chubb, Katy L. and Joseph, Megan and Franklin, Jack and Choudhury, Naail and Furtenbacher, Tibor and Császár, Attila G. and Gaspard, Glenda and Oguoko, Patari and Kelly, Adam and Yurchenko, Sergei N. and Tennyson, Jonathan and Sousa-Silva, Clara},
	urldate = {2020-06-03},
	date = {2018-01-01},
	langid = {english},
	note = {http://web.archive.org/web/20200603225701/https://www.sciencedirect.com/science/article/pii/S0022407317305745},
	file = {ScienceDirect Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\JSPS9265\\S0022407317305745.html:text/html;Chubb et al_2018_MARVEL analysis of the measured high-resolution rovibrational spectra of C2H2.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\B4ETERG7\\Chubb et al_2018_MARVEL analysis of the measured high-resolution rovibrational spectra of C2H2.pdf:application/pdf}
}

@article{furtenbacher_marvel_2007,
	title = {{MARVEL}: measured active rotational–vibrational energy levels},
	volume = {245},
	issn = {0022-2852},
	url = {http://www.sciencedirect.com/science/article/pii/S0022285207002068},
	doi = {10.1016/j.jms.2007.07.005},
	shorttitle = {{MARVEL}},
	abstract = {An algorithm is proposed, based principally on an earlier proposition of Flaud and co-workers [Mol. Phys. 32 (1976) 499], that inverts the information contained in uniquely assigned experimental rotational–vibrational transitions in order to obtain measured active rotational–vibrational energy levels ({MARVEL}). The procedure starts with collecting, critically evaluating, selecting, and compiling all available measured transitions, including assignments and uncertainties, into a single database. Then, spectroscopic networks ({SN}) are determined which contain all interconnecting rotational–vibrational energy levels supported by the grand database of the selected transitions. Adjustment of the uncertainties of the lines is performed next, with the help of a robust weighting strategy, until a self-consistent set of lines and uncertainties is achieved. Inversion of the transitions through a weighted least-squares-type procedure results in {MARVEL} energy levels and associated uncertainties. Local sensitivity coefficients could be computed for each energy level. The resulting set of {MARVEL} levels is called active as when new experimental measurements become available the same evaluation, adjustment, and inversion procedure should be repeated in order to obtain more dependable energy levels and uncertainties. {MARVEL} is tested on the example of the H217O isotopologue of water and a list of 2736 dependable energy levels, based on 8369 transitions, has been obtained.},
	pages = {115--125},
	number = {2},
	journaltitle = {Journal of Molecular Spectroscopy},
	author = {Furtenbacher, Tibor and Császár, Attila G. and Tennyson, Jonathan},
	urldate = {2020-06-03},
	date = {2007-10-01},
	langid = {english},
	keywords = {{MARVEL}, {HO}, Robust fitting, Rotational–vibrational energy levels, Uncertainties},
	file = {ScienceDirect Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\3SSYALEQ\\S0022285207002068.html:text/html;Furtenbacher et al_2007_MARVEL.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\UX4NJUCN\\Furtenbacher et al_2007_MARVEL.pdf:application/pdf}
}

@article{hamilton_inductive_2018,
	title = {Inductive Representation Learning on Large Graphs},
	url = {http://arxiv.org/abs/1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present {GraphSAGE}, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	journaltitle = {{arXiv}:1706.02216 [cs, stat]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2020-06-07},
	date = {2018-09-10},
	eprinttype = {arxiv},
	eprint = {1706.02216},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {Hamilton et al_2018_Inductive Representation Learning on Large Graphs.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\YC9LST8A\\Hamilton et al_2018_Inductive Representation Learning on Large Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\K5LHPB5G\\1706.html:text/html}
}

@article{selsam_learning_2019,
	title = {Learning a {SAT} Solver from Single-Bit Supervision},
	url = {http://arxiv.org/abs/1802.03685},
	abstract = {We present {NeuroSAT}, a message passing neural network that learns to solve {SAT} problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art {SAT} solvers, {NeuroSAT} can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, {NeuroSAT} generalizes to novel distributions; after training only on random {SAT} problems, at test time it can solve {SAT} problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.},
	journaltitle = {{arXiv}:1802.03685 [cs]},
	author = {Selsam, Daniel and Lamm, Matthew and Bünz, Benedikt and Liang, Percy and de Moura, Leonardo and Dill, David L.},
	urldate = {2020-06-07},
	date = {2019-03-11},
	eprinttype = {arxiv},
	eprint = {1802.03685},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Logic in Computer Science},
	file = {Selsam et al_2019_Learning a SAT Solver from Single-Bit Supervision.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\MNAYJF2A\\Selsam et al_2019_Learning a SAT Solver from Single-Bit Supervision.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\F6B6VPXT\\1802.html:text/html}
}

@article{wu_comprehensive_2020,
	title = {A Comprehensive Survey on Graph Neural Networks},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1901.00596},
	doi = {10.1109/TNNLS.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks ({GNNs}) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
	pages = {1--21},
	journaltitle = {{IEEE} Trans. Neural Netw. Learning Syst.},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	urldate = {2020-06-12},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1901.00596},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Wu et al_2020_A Comprehensive Survey on Graph Neural Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\GB9QQAFR\\Wu et al_2020_A Comprehensive Survey on Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\VLB2IAD9\\1901.html:text/html}
}

@article{li_semi-supervised_2019,
	title = {Semi-Supervised Graph Classification: A Hierarchical Graph Perspective},
	url = {http://arxiv.org/abs/1904.05003},
	shorttitle = {Semi-Supervised Graph Classification},
	abstract = {Node classification and graph classification are two graph learning problems that predict the class label of a node and the class label of a graph respectively. A node of a graph usually represents a real-world entity, e.g., a user in a social network, or a protein in a protein-protein interaction network. In this work, we consider a more challenging but practically useful setting, in which a node itself is a graph instance. This leads to a hierarchical graph perspective which arises in many domains such as social network, biological network and document collection. For example, in a social network, a group of people with shared interests forms a user group, whereas a number of user groups are interconnected via interactions or common members. We study the node classification problem in the hierarchical graph where a `node' is a graph instance, e.g., a user group in the above example. As labels are usually limited in real-world data, we design two novel semi-supervised solutions named {\textbackslash}underline\{{SE}\}mi-supervised gr{\textbackslash}underline\{A\}ph c{\textbackslash}underline\{L\}assification via {\textbackslash}underline\{C\}autious/{\textbackslash}underline\{A\}ctive {\textbackslash}underline\{I\}teration (or {SEAL}-C/{AI} in short). {SEAL}-C/{AI} adopt an iterative framework that takes turns to build or update two classifiers, one working at the graph instance level and the other at the hierarchical graph level. To simplify the representation of the hierarchical graph, we propose a novel supervised, self-attentive graph embedding method called {SAGE}, which embeds graph instances of arbitrary size into fixed-length vectors. Through experiments on synthetic data and Tencent {QQ} group data, we demonstrate that {SEAL}-C/{AI} not only outperform competing methods by a significant margin in terms of accuracy/Macro-F1, but also generate meaningful interpretations of the learned representations.},
	journaltitle = {{arXiv}:1904.05003 [cs]},
	author = {Li, Jia and Rong, Yu and Cheng, Hong and Meng, Helen and Huang, Wenbing and Huang, Junzhou},
	urldate = {2020-06-12},
	date = {2019-04-10},
	eprinttype = {arxiv},
	eprint = {1904.05003},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al_2019_Semi-Supervised Graph Classification.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\TS7BYIRT\\Li et al_2019_Semi-Supervised Graph Classification.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\XUQ7FRUU\\1904.html:text/html}
}

@article{de_cao_molgan_2018,
	title = {{MolGAN}: An implicit generative model for small molecular graphs},
	url = {http://arxiv.org/abs/1805.11973},
	shorttitle = {{MolGAN}},
	abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce {MolGAN}, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks ({GANs}) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the {QM}9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. {MolGAN} compares favorably both to recent proposals that use string-based ({SMILES}) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.},
	journaltitle = {{arXiv}:1805.11973 [cs, stat]},
	author = {De Cao, Nicola and Kipf, Thomas},
	urldate = {2020-06-12},
	date = {2018-05-30},
	eprinttype = {arxiv},
	eprint = {1805.11973},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {De Cao_Kipf_2018_MolGAN.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\HTV7VRZT\\De Cao_Kipf_2018_MolGAN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\DMVSERMI\\1805.html:text/html}
}

@article{li_learning_2018,
	title = {Learning Deep Generative Models of Graphs},
	url = {http://arxiv.org/abs/1803.03324},
	abstract = {Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.},
	journaltitle = {{arXiv}:1803.03324 [cs, stat]},
	author = {Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
	urldate = {2020-06-12},
	date = {2018-03-08},
	eprinttype = {arxiv},
	eprint = {1803.03324},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al_2018_Learning Deep Generative Models of Graphs.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\8ZD97GUE\\Li et al_2018_Learning Deep Generative Models of Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\Z949BK9N\\1803.html:text/html}
}

@article{bojchevski_netgan_2018,
	title = {{NetGAN}: Generating Graphs via Random Walks},
	url = {http://arxiv.org/abs/1803.00816},
	shorttitle = {{NetGAN}},
	abstract = {We propose {NetGAN} - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein {GAN} objective. {NetGAN} is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, {NetGAN} opens exciting avenues for further research.},
	journaltitle = {{arXiv}:1803.00816 [cs, stat]},
	author = {Bojchevski, Aleksandar and Shchur, Oleksandr and Zügner, Daniel and Günnemann, Stephan},
	urldate = {2020-06-12},
	date = {2018-06-01},
	eprinttype = {arxiv},
	eprint = {1803.00816},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {Bojchevski et al_2018_NetGAN.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\D9AECDDB\\Bojchevski et al_2018_NetGAN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\X3ZXQKRW\\1803.html:text/html}
}

@article{you_graphrnn_2018,
	title = {{GraphRNN}: Generating Realistic Graphs with Deep Auto-regressive Models},
	url = {http://arxiv.org/abs/1802.08773},
	shorttitle = {{GraphRNN}},
	abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose {GraphRNN}, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. {GraphRNN} learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of {GraphRNN}, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that {GraphRNN} significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
	journaltitle = {{arXiv}:1802.08773 [cs]},
	author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	urldate = {2020-06-12},
	date = {2018-06-23},
	eprinttype = {arxiv},
	eprint = {1802.08773},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, I.2.6},
	file = {You et al_2018_GraphRNN.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\CIEZ9NXH\\You et al_2018_GraphRNN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\RHM2ZGLV\\1802.html:text/html}
}

@article{watters_visual_2017,
	title = {Visual Interaction Networks},
	url = {http://arxiv.org/abs/1706.01433},
	abstract = {From just a glance, humans can make rich predictions about the future state of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains and require direct measurements of the underlying states. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions and dynamics, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. Our results demonstrate that the perceptual module and the object-based dynamics predictor module can induce factored latent representations that support accurate dynamical predictions. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.},
	journaltitle = {{arXiv}:1706.01433 [cs]},
	author = {Watters, Nicholas and Tacchetti, Andrea and Weber, Theophane and Pascanu, Razvan and Battaglia, Peter and Zoran, Daniel},
	urldate = {2020-06-12},
	date = {2017-06-05},
	eprinttype = {arxiv},
	eprint = {1706.01433},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Watters et al_2017_Visual Interaction Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\V2GXWN97\\Watters et al_2017_Visual Interaction Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\8S4U8J4R\\1706.html:text/html}
}

@article{kipf_neural_2018,
	title = {Neural Relational Inference for Interacting Systems},
	url = {http://arxiv.org/abs/1802.04687},
	abstract = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference ({NRI}) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our {NRI} model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.},
	journaltitle = {{arXiv}:1802.04687 [cs, stat]},
	author = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
	urldate = {2020-06-12},
	date = {2018-06-06},
	eprinttype = {arxiv},
	eprint = {1802.04687},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Kipf et al_2018_Neural Relational Inference for Interacting Systems.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\ADJBNS35\\Kipf et al_2018_Neural Relational Inference for Interacting Systems.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\B8AZ7EPI\\1802.html:text/html}
}

@article{zhang_general_2019,
	title = {A General Deep Learning Framework for Network Reconstruction and Dynamics Learning},
	volume = {4},
	issn = {2364-8228},
	url = {http://arxiv.org/abs/1812.11482},
	doi = {10.1007/s41109-019-0194-4},
	abstract = {Many complex processes can be viewed as dynamical systems on networks. However, in real cases, only the performances of the system are known, the network structure and the dynamical rules are not observed. Therefore, recovering latent network structure and dynamics from observed time series data are important tasks because it may help us to open the black box, and even to build up the model of a complex system automatically. Although this problem hosts a wealth of potential applications in biology, earth science, and epidemics etc., conventional methods have limitations. In this work, we introduce a new framework, Gumbel Graph Network ({GGN}), which is a model-free, data-driven deep learning framework to accomplish the reconstruction of both network connections and the dynamics on it. Our model consists of two jointly trained parts: a network generator that generating a discrete network with the Gumbel Softmax technique; and a dynamics learner that utilizing the generated network and one-step trajectory value to predict the states in future steps. We exhibit the universality of our framework on different kinds of time-series data: with the same structure, our model can be trained to accurately recover the network structure and predict future states on continuous, discrete, and binary dynamics, and outperforms competing network reconstruction methods.},
	pages = {110},
	number = {1},
	journaltitle = {Appl Netw Sci},
	author = {Zhang, Zhang and Zhao, Yi and Liu, Jing and Wang, Shuo and Tao, Ruyi and Xin, Ruyue and Zhang, Jiang},
	urldate = {2020-06-12},
	date = {2019-12},
	eprinttype = {arxiv},
	eprint = {1812.11482},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Condensed Matter - Disordered Systems and Neural Networks, Physics - Physics and Society},
	file = {Zhang et al_2019_A General Deep Learning Framework for Network Reconstruction and Dynamics.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\4JGLL77S\\Zhang et al_2019_A General Deep Learning Framework for Network Reconstruction and Dynamics.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\BTKYCH28\\1812.html:text/html}
}

@article{jang_categorical_2017,
	title = {Categorical Reparameterization with Gumbel-Softmax},
	url = {http://arxiv.org/abs/1611.01144},
	abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
	journaltitle = {{arXiv}:1611.01144 [cs, stat]},
	author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
	urldate = {2020-06-14},
	date = {2017-08-05},
	eprinttype = {arxiv},
	eprint = {1611.01144},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Jang et al_2017_Categorical Reparameterization with Gumbel-Softmax.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\H3NPU2YA\\Jang et al_2017_Categorical Reparameterization with Gumbel-Softmax.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\TFB9UYEC\\1611.html:text/html}
}

@article{sanchez-gonzalez_learning_2020,
	title = {Learning to Simulate Complex Physics with Graph Networks},
	url = {http://arxiv.org/abs/2002.09405},
	abstract = {Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" ({GNS})---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our {GNS} framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.},
	journaltitle = {{arXiv}:2002.09405 [physics, stat]},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	urldate = {2020-06-23},
	date = {2020-02-21},
	eprinttype = {arxiv},
	eprint = {2002.09405},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics},
	file = {Sanchez-Gonzalez et al_2020_Learning to Simulate Complex Physics with Graph Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\BA2MP3UJ\\Sanchez-Gonzalez et al_2020_Learning to Simulate Complex Physics with Graph Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\SY3RQ36G\\2002.html:text/html}
}

@article{harary_dynamic_1997,
	title = {Dynamic graph models},
	volume = {25},
	issn = {0895-7177},
	url = {http://www.sciencedirect.com/science/article/pii/S0895717797000502},
	doi = {10.1016/S0895-7177(97)00050-2},
	abstract = {Research in graph theory has focused on studying the structure of graphs with the assumption that they are static. However, in many applications, the graphs that arise change with time, i.e., they are dynamic in nature. This is especially true of applications involving graph models in computer science. We present an expository study of dynamic graphs with the main driving force being practical applications. We first develop a formal classification of dynamic graphs. This taxonomy in the form of generalizations and extensions will in turn suggest new areas of application. Next, we discuss areas where dynamic graphs arise in computer science such as compilers, databases, fault-tolerance, artificial intelligence, and computer networks. Finally, we propose approaches that can be used for studying dynamic graphs. The main objective in any study of dynamic graphs should be to 1.(i) extend results developed for static graph theory to dynamic graphs,2.(ii) study the properties that describe how a dynamic graph changes,3.(iii) investigate problems and issues in dynamic graph theory that are raised by practical applications of dynamic graphs in computer science.},
	pages = {79--87},
	number = {7},
	journaltitle = {Mathematical and Computer Modelling},
	author = {Harary, F. and Gupta, G.},
	urldate = {2020-06-27},
	date = {1997-04-01},
	langid = {english},
	keywords = {Dynamic graphs, Graph models in computer science, Graph theory},
	file = {ScienceDirect Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\WES7TGJ8\\S0895717797000502.html:text/html;Harary_Gupta_1997_Dynamic graph models.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\84EPJXVA\\Harary_Gupta_1997_Dynamic graph models.pdf:application/pdf}
}

@inproceedings{pal_deep_2016,
	title = {Deep learning for network analysis: Problems, approaches and challenges},
	url = {10.1109/MILCOM.2016.7795391},
	doi = {10.1109/MILCOM.2016.7795391},
	shorttitle = {Deep learning for network analysis},
	abstract = {The analysis of social, communication and information networks for identifying patterns, evolutionary characteristics and anomalies is a key problem for the military, for instance in the Intelligence community. Current techniques do not have the ability to discern unusual features or patterns that are not a priori known. We investigate the use of deep learning for network analysis. Over the last few years, deep learning has had unprecedented success in areas such as image classification, speech recognition, etc. However, research on the use of deep learning to network or graph analysis is limited. We present three preliminary techniques that we have developed as part of the {ARL} Network Science {CTA} program: (a) unsupervised classification using a very highly trained image recognizer, namely Caffe; (b) supervised classification using a variant of convolutional neural networks on node features such as degree and assortativity; and (c) a framework called node2vec for learning representations of nodes in a network using a mapping to natural language processing.},
	eventtitle = {{MILCOM} 2016 - 2016 {IEEE} Military Communications Conference},
	pages = {588--593},
	booktitle = {{MILCOM} 2016 - 2016 {IEEE} Military Communications Conference},
	author = {Pal, Siddharth and Dong, Yuxiao and Thapa, Bishal and Chawla, Nitesh V. and Swami, Ananthram and Ramanathan, Ram},
	date = {2016-11},
	note = {{ISSN}: 2155-7586},
	keywords = {Machine learning, Biological neural networks, Convolution, convolutional neural network, data mining, deep learning, Erbium, Feature extraction, graph theory, image recognizer, intelligence community, learning (artificial intelligence), Measurement, military computing, military system, network analysis, network theory (graphs), neural nets, node2vec framework, pattern classification, unsupervised classification},
	file = {Pal et al_2016_Deep learning for network analysis.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\YNAIVJ9D\\Pal et al_2016_Deep learning for network analysis.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\mihasik2654\\Zotero\\storage\\RQQXX4QE\\7795391.html:text/html}
}

@article{schlichtkrull_modeling_2017,
	title = {Modeling Relational Data with Graph Convolutional Networks},
	url = {http://arxiv.org/abs/1703.06103},
	abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, {DBPedia} or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-{GCNs}) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-{GCNs} are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-{GCNs} as a stand-alone model for entity classification. We further show that factorization models for link prediction such as {DistMult} can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8\% on {FB}15k-237 over a decoder-only baseline.},
	journaltitle = {{arXiv}:1703.06103 [cs, stat]},
	author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and Berg, Rianne van den and Titov, Ivan and Welling, Max},
	urldate = {2020-07-04},
	date = {2017-10-26},
	eprinttype = {arxiv},
	eprint = {1703.06103},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Databases},
	file = {Schlichtkrull et al_2017_Modeling Relational Data with Graph Convolutional Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\6NBPDP3P\\Schlichtkrull et al_2017_Modeling Relational Data with Graph Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\DI7UHGPV\\1703.html:text/html}
}

@article{tang_chebnet_2019,
	title = {{ChebNet}: Efficient and Stable Constructions of Deep Neural Networks with Rectified Power Units using Chebyshev Approximations},
	url = {http://arxiv.org/abs/1911.05467},
	shorttitle = {{ChebNet}},
	abstract = {In a recent paper [B. Li, S. Tang and H. Yu, {arXiv}:1903.05858], it was shown that deep neural networks built with rectified power units ({RePU}) can give better approximation for sufficient smooth functions than those with rectified linear units, by converting polynomial approximation given in power series into deep neural networks with optimal complexity and no approximation error. However, in practice, power series are not easy to compute. In this paper, we propose a new and more stable way to construct deep {RePU} neural networks based on Chebyshev polynomial approximations. By using a hierarchical structure of Chebyshev polynomial approximation in frequency domain, we build efficient and stable deep neural network constructions. In theory, {ChebNets} and the deep {RePU} nets based on Power series have the same upper error bounds for general function approximations. But numerically, {ChebNets} are much more stable. Numerical results show that the constructed {ChebNets} can be further trained and obtain much better results than those obtained by training deep {RePU} nets constructed basing on power series.},
	journaltitle = {{arXiv}:1911.05467 [cs, math]},
	author = {Tang, Shanshan and Li, Bo and Yu, Haijun},
	urldate = {2020-07-04},
	date = {2019-12-20},
	eprinttype = {arxiv},
	eprint = {1911.05467},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {Tang et al_2019_ChebNet.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\2IGM4LFK\\Tang et al_2019_ChebNet.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\JAZPYJJC\\1911.html:text/html}
}

@article{bengio_scheduled_2015,
	title = {Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1506.03099},
	abstract = {Recurrent Neural Networks can be trained to produce sequences of tokens given some input, as exemplified by recent results in machine translation and image captioning. The current approach to training them consists of maximizing the likelihood of each token in the sequence given the current (recurrent) state and the previous token. At inference, the unknown previous token is then replaced by a token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence. We propose a curriculum learning strategy to gently change the training process from a fully guided scheme using the true previous token, towards a less guided scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements. Moreover, it was used successfully in our winning entry to the {MSCOCO} image captioning challenge, 2015.},
	journaltitle = {{arXiv}:1506.03099 [cs]},
	author = {Bengio, Samy and Vinyals, Oriol and Jaitly, Navdeep and Shazeer, Noam},
	urldate = {2020-07-04},
	date = {2015-09-23},
	eprinttype = {arxiv},
	eprint = {1506.03099},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Bengio et al_2015_Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\2KT5X5HY\\Bengio et al_2015_Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\QBZVJVCS\\1506.html:text/html}
}

@online{noauthor_neutm_nodate,
	title = {{NeuTM}: A neural network-based framework for traffic matrix prediction in {SDN} - {IEEE} Conference Publication},
	url = {https://ieeexplore.ieee.org/document/8406199},
	urldate = {2020-07-05},
	note = {http://web.archive.org/web/20200705143331/https://ieeexplore.ieee.org/document/8406199},
	file = {NeuTM\: A neural network-based framework for traffic matrix prediction in SDN - IEEE Conference Publication:C\:\\Users\\mihasik2654\\Zotero\\storage\\MRZGE74G\\8406199.html:text/html}
}

@inproceedings{azzouni_neutm_2018,
	title = {{NeuTM}: A neural network-based framework for traffic matrix prediction in {SDN}},
	url = {10.1109/NOMS.2018.8406199},
	doi = {10.1109/NOMS.2018.8406199},
	shorttitle = {{NeuTM}},
	abstract = {This paper presents {NeuTM}, a framework for network Traffic Matrix ({TM}) prediction based on Long Short-Term Memory Recurrent Neural Networks ({LSTM} {RNNs}). {TM} prediction is defined as the problem of estimating future network traffic matrix from the previous and achieved network traffic data. It is widely used in network planning, resource management and network security. Long Short-Term Memory ({LSTM}) is a specific recurrent neural network ({RNN}) architecture that is well-suited to learn from data and classify or predict time series with time lags of unknown size. {LSTMs} have been shown to model longrange dependencies more accurately than conventional {RNNs}. {NeuTM} is a {LSTM} {RNN}-based framework for predicting {TM} in large networks. By validating our framework on real-world data from {GEANT} network, we show that our model converges quickly and gives state of the art {TM} prediction performance.},
	eventtitle = {{NOMS} 2018 - 2018 {IEEE}/{IFIP} Network Operations and Management Symposium},
	pages = {1--5},
	booktitle = {{NOMS} 2018 - 2018 {IEEE}/{IFIP} Network Operations and Management Symposium},
	author = {Azzouni, Abdelhadi and Pujolle, Guy},
	date = {2018-04},
	note = {{ISSN}: 2374-9709},
	keywords = {Recurrent neural networks, learning (artificial intelligence), Artificial neural networks, Computer architecture, {GEANT} network, Logic gates, long short term memory recurrent neural networks, Long Short-Term Mermory, {LSTM} {RNNs}, matrix algebra, network security, network traffic data, network traffic matrix prediction, Neural Networks, {NeuTM}, Prediction, Predictive models, recurrent neural nets, resource management, software defined networking, Software Defined Networking, telecommunication traffic, time series, Time series analysis, time series prediction, Traffic Matrix},
	file = {Azzouni_Pujolle_2018_NeuTM.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\DBS4DYGW\\Azzouni_Pujolle_2018_NeuTM.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\mihasik2654\\Zotero\\storage\\LCQ44RWH\\8406199.html:text/html}
}

@article{perraudin_deepsphere_2019,
	title = {{DeepSphere}: Efficient spherical convolutional neural network with {HEALPix} sampling for cosmological applications},
	volume = {27},
	issn = {2213-1337},
	url = {http://www.sciencedirect.com/science/article/pii/S2213133718301392},
	doi = {10.1016/j.ascom.2019.03.004},
	shorttitle = {{DeepSphere}},
	abstract = {Convolutional Neural Networks ({CNNs}) are a cornerstone of the Deep Learning toolbox and have led to many breakthroughs in Artificial Intelligence. So far, these neural networks ({NNs}) have mostly been developed for regular Euclidean domains such as those supporting images, audio, or video. Because of their success, {CNN}-based methods are becoming increasingly popular in Cosmology. Cosmological data often comes as spherical maps, which make the use of the traditional {CNNs} more complicated. The commonly used pixelization scheme for spherical maps is the Hierarchical Equal Area {isoLatitude} Pixelisation ({HEALPix}). We present a spherical {CNN} for analysis of full and partial {HEALPix} maps, which we call {DeepSphere}. The spherical {CNN} is constructed by representing the sphere as a graph. Graphs are versatile data structures that can represent pairwise relationships between objects or act as a discrete representation of a continuous manifold. Using the graph-based representation, we define many of the standard {CNN} operations, such as convolution and pooling. With filters restricted to being radial, our convolutions are equivariant to rotation on the sphere, and {DeepSphere} can be made invariant or equivariant to rotation. This way, {DeepSphere} is a special case of a graph {CNN}, tailored to the {HEALPix} sampling of the sphere. This approach is computationally more efficient than using spherical harmonics to perform convolutions. We demonstrate the method on a classification problem of weak lensing mass maps from two cosmological models and compare its performance with that of three baseline classifiers, two based on the power spectrum and pixel density histogram, and a classical 2D {CNN}. Our experimental results show that the performance of {DeepSphere} is always superior or equal to the baselines. For high noise levels and for data covering only a smaller fraction of the sphere, {DeepSphere} achieves typically 10\% better classification accuracy than the baselines.Finally, we show how learned filters can be visualized to introspect the {NN}. Code and examples are available at https://github.com/{SwissDataScienceCenter}/{DeepSphere}.},
	pages = {130--146},
	journaltitle = {Astronomy and Computing},
	author = {Perraudin, N. and Defferrard, M. and Kacprzak, T. and Sgier, R.},
	urldate = {2020-07-05},
	date = {2019-04-01},
	langid = {english},
	keywords = {Cosmological data analysis, {DeepSphere}, Graph {CNN}, Mass mapping, Spherical convolutional neural network},
	file = {ScienceDirect Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\Q37U7DNN\\S2213133718301392.html:text/html;Perraudin et al_2019_DeepSphere.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\DAL4NWK6\\Perraudin et al_2019_DeepSphere.pdf:application/pdf}
}

@article{zhang_spatial-temporal_2019,
	title = {Spatial-Temporal Graph Attention Networks: A Deep Learning Approach for Traffic Forecasting},
	volume = {7},
	issn = {2169-3536},
	url = {10.1109/ACCESS.2019.2953888},
	doi = {10.1109/ACCESS.2019.2953888},
	shorttitle = {Spatial-Temporal Graph Attention Networks},
	abstract = {Traffic speed prediction, as one of the most important topics in Intelligent Transport Systems ({ITS}), has been investigated thoroughly in the literature. Nonetheless, traditional methods show their limitation in coping with complexity and high nonlinearity of traffic data as well as learning spatial-temporal dependencies. Particularly, they often neglect the dynamics happening to traffic network. Attention-based models witnessed extensive developments in recent years and have shown its efficacy in a host of fields, which inspires us to leverage graph-attention-based method to handling traffic network speed prediction. In this paper, we propose a novel deep learning framework, Spatial-Temporal Graph Attention Networks ({ST}-{GAT}). A graph attention mechanism is adopted to extract the spatial dependencies among road segments. Additionally, we introduce a {LSTM} network to extract temporal domain features. Compared with previous related research, the proposed approach is able to capture dynamic spatial dependencies of traffic networks. A series of comprehensive case studies on a real-world dataset demonstrate that {ST}-{GAT} supersedes existing state-of-the-art results of traffic speed prediction. Furthermore, outstanding robustness against noise and on reduced graphs of the proposed model has been demonstrated through the tests.},
	pages = {166246--166256},
	journaltitle = {{IEEE} Access},
	author = {Zhang, Chenhan and Yu, James J. Q. and Liu, Yi},
	date = {2019},
	note = {Conference Name: {IEEE} Access},
	keywords = {Deep learning, deep learning, Feature extraction, graph theory, learning (artificial intelligence), Predictive models, Computational modeling, deep learning approach, dynamic spatial dependencies, feature extraction, Forecasting, graph attention, graph-attention-based method, intelligent transport systems, intelligent transportation system, intelligent transportation systems, {ITS}, learning spatial-temporal dependencies, {LSTM} network, Mathematical model, road segments, road traffic, Roads, spatial-temporal graph attention networks, spatio-temporal domain feature, {ST}-{GAT}, temporal domain features extraction, traffic data, traffic engineering computing, traffic forecasting, traffic network speed prediction, Traffic speed prediction},
	file = {Zhang et al_2019_Spatial-Temporal Graph Attention Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\YRP3XSDN\\Zhang et al_2019_Spatial-Temporal Graph Attention Networks.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\mihasik2654\\Zotero\\storage\\CMLZ4R2M\\8903252.html:text/html}
}

@inproceedings{andreoletti_network_2019,
	title = {Network Traffic Prediction based on Diffusion Convolutional Recurrent Neural Networks},
	url = {10.1109/INFCOMW.2019.8845132},
	doi = {10.1109/INFCOMW.2019.8845132},
	abstract = {By predicting the traffic load on network links, a network operator can effectively pre-dispose resource-allocation strategies to early address, e.g., an incoming congestion event. Traffic loads on different links of a telecom is know to be subject to strong correlation, and this correlation, if properly represented, can be exploited to refine the prediction of future congestion events. Machine Learning ({ML}) represents nowadays the state-of-the-art methodology for discovering complex relations among data. However, {ML} has been traditionally applied to data represented in the Euclidean space (e.g., to images) and it may not be straightforward to effectively employ it to model graph-stuctured data (e.g., as the events that take place in telecom networks). Recently, several {ML} algorithms specifically designed to learn models of graph-structured data have appeared in the literature. The main novelty of these techniques relies on their ability to learn a representation of each node of the graph considering both its properties (e.g., features) and the structure of the network (e.g., the topology). In this paper, we employ a recently-proposed graph-based {ML} algorithm, the Diffusion Convolutional Recurrent Neural Network ({DCRNN}), to forecast traffic load on the links of a real backbone network. We evaluate {DRCNN}'s ability to forecast the volume of expected traffic and to predict events of congestion, and we compare this approach to other existing approaches (as {LSTM}, and Fully-Connected Neural Networks). Results show that {DCRN} outperforms the other methods both in terms of its forecasting ability (e.g., {MAPE} is reduced from 210\% to 43\%) and in terms of the prediction of congestion events, and represent promising starting point for the application of {DRCNN} to other network management problems.},
	eventtitle = {{IEEE} {INFOCOM} 2019 - {IEEE} Conference on Computer Communications Workshops ({INFOCOM} {WKSHPS})},
	pages = {246--251},
	booktitle = {{IEEE} {INFOCOM} 2019 - {IEEE} Conference on Computer Communications Workshops ({INFOCOM} {WKSHPS})},
	author = {Andreoletti, Davide and Troia, Sebastian and Musumeci, Francesco and Giordano, Silvia and Maier, Guido and Tornatore, Massimo},
	date = {2019-04},
	keywords = {Machine learning, Recurrent neural networks, Convolution, learning (artificial intelligence), network theory (graphs), Neural Networks, recurrent neural nets, telecommunication traffic, traffic forecasting, backbone network, computer network management, convolutional neural nets, Correlation, Data models, Diffusion Convolutional Recurrent Neural Network, graph-based machine learning, graph-structured data, Machine learning algorithms, {ML} algorithms, network congestion, network links, network management, network operator, Network traffic, resource allocation, resource-allocation strategies, telecom networks, telecommunication congestion control, telecommunication network topology, Telecommunications},
	file = {Andreoletti et al_2019_Network Traffic Prediction based on Diffusion Convolutional Recurrent Neural.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\8D5MGJJS\\Andreoletti et al_2019_Network Traffic Prediction based on Diffusion Convolutional Recurrent Neural.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\mihasik2654\\Zotero\\storage\\NLJYJVHH\\8845132.html:text/html}
}

@article{salinas_deepar_2020,
	title = {{DeepAR}: Probabilistic forecasting with autoregressive recurrent networks},
	volume = {36},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207019301888},
	doi = {10.1016/j.ijforecast.2019.07.001},
	shorttitle = {{DeepAR}},
	abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes {DeepAR}, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.},
	pages = {1181--1191},
	number = {3},
	journaltitle = {International Journal of Forecasting},
	author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim},
	urldate = {2020-07-05},
	date = {2020-07-01},
	langid = {english},
	keywords = {Big data, Deep learning, Neural networks, Demand forecasting, Probabilistic forecasting},
	file = {ScienceDirect Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\48X3W8US\\S0169207019301888.html:text/html;Salinas et al_2020_DeepAR.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\HDW5LC2A\\Salinas et al_2020_DeepAR.pdf:application/pdf}
}

@article{velickovic_graph_2018,
	title = {Graph Attention Networks},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks ({GATs}), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our {GAT} models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	journaltitle = {{arXiv}:1710.10903 [cs, stat]},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	urldate = {2020-07-05},
	date = {2018-02-04},
	eprinttype = {arxiv},
	eprint = {1710.10903},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {Veličković et al_2018_Graph Attention Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\AMSXQ5XZ\\Veličković et al_2018_Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\P4JA4QZR\\1710.html:text/html}
}

@article{hoshen_vain_2018,
	title = {{VAIN}: Attentional Multi-agent Predictive Modeling},
	url = {http://arxiv.org/abs/1706.06122},
	shorttitle = {{VAIN}},
	abstract = {Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks ({INs}) were proposed for the task of modeling multi-agent physical systems, {INs} scale with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce {VAIN}, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that {VAIN} is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.},
	journaltitle = {{arXiv}:1706.06122 [cs]},
	author = {Hoshen, Yedid},
	urldate = {2020-07-05},
	date = {2018-09-28},
	eprinttype = {arxiv},
	eprint = {1706.06122},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Hoshen_2018_VAIN.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\D32EQ5N5\\Hoshen_2018_VAIN.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\VKJ79PIW\\1706.html:text/html}
}

@article{van_steenkiste_relational_2018,
	title = {Relational Neural Expectation Maximization: Unsupervised Discovery of Objects and their Interactions},
	url = {http://arxiv.org/abs/1802.10353},
	shorttitle = {Relational Neural Expectation Maximization},
	abstract = {Common-sense physical reasoning is an essential ingredient for any intelligent agent operating in the real-world. For example, it can be used to simulate the environment, or to infer the state of parts of the world that are currently unobserved. In order to match real-world conditions this causal knowledge must be learned without access to supervised data. To address this problem we present a novel method that learns to discover objects and model their physical interactions from raw visual images in a purely {\textbackslash}emph\{unsupervised\} fashion. It incorporates prior knowledge about the compositional nature of human perception to factor interactions between object-pairs and learn efficiently. On videos of bouncing balls we show the superior modelling capabilities of our method compared to other unsupervised neural approaches that do not incorporate such prior knowledge. We demonstrate its ability to handle occlusion and show that it can extrapolate learned knowledge to scenes with different numbers of objects.},
	journaltitle = {{arXiv}:1802.10353 [cs]},
	author = {van Steenkiste, Sjoerd and Chang, Michael and Greff, Klaus and Schmidhuber, Jürgen},
	urldate = {2020-07-05},
	date = {2018-02-28},
	eprinttype = {arxiv},
	eprint = {1802.10353},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.2.6, Computer Science - Neural and Evolutionary Computing},
	file = {van Steenkiste et al_2018_Relational Neural Expectation Maximization.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\9CTBK2JS\\van Steenkiste et al_2018_Relational Neural Expectation Maximization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\W8B6TQQJ\\1802.html:text/html}
}

@article{cranmer_discovering_2020,
	title = {Discovering Symbolic Models from Deep Learning with Inductive Biases},
	url = {http://arxiv.org/abs/2006.11287},
	abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks ({GNNs}). The technique works as follows: we first encourage sparse latent representations when we train a {GNN} in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the {GNN} using our technique also generalized to out-of-distribution data better than the {GNN} itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
	journaltitle = {{arXiv}:2006.11287 [astro-ph, physics:physics, stat]},
	author = {Cranmer, Miles and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
	urldate = {2020-07-06},
	date = {2020-06-19},
	eprinttype = {arxiv},
	eprint = {2006.11287},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {Cranmer et al_2020_Discovering Symbolic Models from Deep Learning with Inductive Biases.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\VQX9BSCI\\Cranmer et al_2020_Discovering Symbolic Models from Deep Learning with Inductive Biases.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\IZXFGF7S\\2006.html:text/html}
}

@article{cranmer_lagrangian_2020,
	title = {Lagrangian Neural Networks},
	url = {http://arxiv.org/abs/2003.04630},
	abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks ({LNNs}), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, {LNNs} do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.},
	journaltitle = {{arXiv}:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	urldate = {2020-07-06},
	date = {2020-03-10},
	eprinttype = {arxiv},
	eprint = {2003.04630},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics, Mathematics - Dynamical Systems, Physics - Data Analysis, Statistics and Probability},
	file = {Cranmer et al_2020_Lagrangian Neural Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\V4NU5ND2\\Cranmer et al_2020_Lagrangian Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\26XSG7DG\\2003.html:text/html}
}

@article{murphy_deep_2020,
	title = {Deep learning of stochastic contagion dynamics on complex networks},
	url = {http://arxiv.org/abs/2006.05410},
	abstract = {Forecasting the evolution of contagion dynamics is still an open problem to which mechanistic models only offer a partial answer. To remain mathematically and/or computationally tractable, these models must rely on simplifying assumptions, thereby limiting the quantitative accuracy of their predictions and the complexity of the dynamics they can model. Here, we propose a complementary approach based on deep learning where the effective local mechanisms governing a dynamic are learned automatically from time series data. Our graph neural network architecture makes very few assumptions about the dynamics, and we demonstrate its accuracy using stochastic contagion dynamics of increasing complexity on static and temporal networks. By allowing simulations on arbitrary network structures, our approach makes it possible to explore the properties of the learned dynamics beyond the training data. Our results demonstrate how deep learning offers a new and complementary perspective to build effective models of contagion dynamics on networks.},
	journaltitle = {{arXiv}:2006.05410 [cond-mat, physics:physics, stat]},
	author = {Murphy, Charles and Laurence, Edward and Allard, Antoine},
	urldate = {2020-07-06},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2006.05410},
	keywords = {Statistics - Machine Learning, Physics - Physics and Society, Condensed Matter - Statistical Mechanics},
	file = {Murphy et al_2020_Deep learning of stochastic contagion dynamics on complex networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\XVUWM3KE\\Murphy et al_2020_Deep learning of stochastic contagion dynamics on complex networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\ZTQIQXSQ\\2006.html:text/html}
}

@article{zhang_general_2019-1,
	title = {A General Deep Learning Framework for Network Reconstruction and Dynamics Learning},
	volume = {4},
	issn = {2364-8228},
	url = {http://arxiv.org/abs/1812.11482},
	doi = {10.1007/s41109-019-0194-4},
	abstract = {Many complex processes can be viewed as dynamical systems on networks. However, in real cases, only the performances of the system are known, the network structure and the dynamical rules are not observed. Therefore, recovering latent network structure and dynamics from observed time series data are important tasks because it may help us to open the black box, and even to build up the model of a complex system automatically. Although this problem hosts a wealth of potential applications in biology, earth science, and epidemics etc., conventional methods have limitations. In this work, we introduce a new framework, Gumbel Graph Network ({GGN}), which is a model-free, data-driven deep learning framework to accomplish the reconstruction of both network connections and the dynamics on it. Our model consists of two jointly trained parts: a network generator that generating a discrete network with the Gumbel Softmax technique; and a dynamics learner that utilizing the generated network and one-step trajectory value to predict the states in future steps. We exhibit the universality of our framework on different kinds of time-series data: with the same structure, our model can be trained to accurately recover the network structure and predict future states on continuous, discrete, and binary dynamics, and outperforms competing network reconstruction methods.},
	pages = {110},
	number = {1},
	journaltitle = {Appl Netw Sci},
	author = {Zhang, Zhang and Zhao, Yi and Liu, Jing and Wang, Shuo and Tao, Ruyi and Xin, Ruyue and Zhang, Jiang},
	urldate = {2020-07-06},
	date = {2019-12},
	eprinttype = {arxiv},
	eprint = {1812.11482},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Condensed Matter - Disordered Systems and Neural Networks, Physics - Physics and Society},
	file = {Zhang et al_2019_A General Deep Learning Framework for Network Reconstruction and Dynamics.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\3UD35ESA\\Zhang et al_2019_A General Deep Learning Framework for Network Reconstruction and Dynamics.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\RXAPEL2L\\1812.html:text/html}
}

@article{chen_inference_2020,
	title = {Inference for Network Structure and Dynamics from Time Series Data via Graph Neural Network},
	url = {http://arxiv.org/abs/2001.06576},
	abstract = {Network structures in various backgrounds play important roles in social, technological, and biological systems. However, the observable network structures in real cases are often incomplete or unavailable due to measurement errors or private protection issues. Therefore, inferring the complete network structure is useful for understanding complex systems. The existing studies have not fully solved the problem of inferring network structure with partial or no information about connections or nodes. In this paper, we tackle the problem by utilizing time series data generated by network dynamics. We regard the network inference problem based on dynamical time series data as a problem of minimizing errors for predicting future states and proposed a novel data-driven deep learning model called Gumbel Graph Network ({GGN}) to solve the two kinds of network inference problems: Network Reconstruction and Network Completion. For the network reconstruction problem, the {GGN} framework includes two modules: the dynamics learner and the network generator. For the network completion problem, {GGN} adds a new module called the States Learner to infer missing parts of the network. We carried out experiments on discrete and continuous time series data. The experiments show that our method can reconstruct up to 100\% network structure on the network reconstruction task. While the model can also infer the unknown parts of the structure with up to 90\% accuracy when some nodes are missing. And the accuracy decays with the increase of the fractions of missing nodes. Our framework may have wide application areas where the network structure is hard to obtained and the time series data is rich.},
	journaltitle = {{arXiv}:2001.06576 [cs, stat]},
	author = {Chen, Mengyuan and Zhang, Jiang and Zhang, Zhang and Du, Lun and Hu, Qiao and Wang, Shuo and Zhu, Jiaqi},
	urldate = {2020-07-06},
	date = {2020-01-17},
	eprinttype = {arxiv},
	eprint = {2001.06576},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {Chen et al_2020_Inference for Network Structure and Dynamics from Time Series Data via Graph.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\LNQ554GJ\\Chen et al_2020_Inference for Network Structure and Dynamics from Time Series Data via Graph.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\GUFF98EY\\2001.html:text/html}
}

@article{zhang_neural_2019,
	title = {Neural Gene Network Constructor: A Neural Based Model for Reconstructing Gene Regulatory Network},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {10.1101/842369},
	doi = {10.1101/842369},
	shorttitle = {Neural Gene Network Constructor},
	abstract = {{\textless}h3{\textgreater}Summary{\textless}/h3{\textgreater} {\textless}p{\textgreater}Reconstructing gene regulatory networks ({GRNs}) and inferring the gene dynamics are important to understand the behavior and the fate of the normal and abnormal cells. Gene regulatory networks could be reconstructed by experimental methods or from gene expression data. Recent advances in Single Cell {RNA} sequencing technology and the computational method to reconstruct trajectory have generated huge {scRNA}-seq data tagged with additional time labels. Here, we present a deep learning model “Neural Gene Network Constructor” ({NGNC}), for inferring gene regulatory network and reconstructing the gene dynamics simultaneously from time series gene expression data. {NGNC} is a model-free heterogenous model, which can reconstruct any network structure and non-linear dynamics. It consists of two parts: a network generator which incorporating gumbel softmax technique to generate candidate network structure, and a dynamics learner which adopting multiple feedforward neural networks to predict the dynamics. We compare our model with other well-known frameworks on the data set generated by {GeneNetWeaver}, and achieve the state of the arts results both on network reconstruction and dynamics learning.{\textless}/p{\textgreater}},
	pages = {842369},
	journaltitle = {{bioRxiv}},
	author = {Zhang, Zhang and Wang, Lifei and Wang, Shuo and Tao, Ruyi and Xiao, Jingshu and Mou, Muyun and Cai, Jun and Zhang, Jiang},
	urldate = {2020-07-06},
	date = {2019-11-14},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results; http://web.archive.org/web/20200706190748/https://www.biorxiv.org/content/10.1101/842369v1},
	file = {Zhang et al_2019_Neural Gene Network Constructor.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\JIFE6ICR\\Zhang et al_2019_Neural Gene Network Constructor.pdf:application/pdf;Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\G5EHT9NF\\842369v1.html:text/html}
}

@article{grover_node2vec_2016,
	title = {node2vec: Scalable Feature Learning for Networks},
	url = {http://arxiv.org/abs/1607.00653},
	shorttitle = {node2vec},
	abstract = {Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.},
	journaltitle = {{arXiv}:1607.00653 [cs, stat]},
	author = {Grover, Aditya and Leskovec, Jure},
	urldate = {2020-07-06},
	date = {2016-07-03},
	eprinttype = {arxiv},
	eprint = {1607.00653},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {Grover_Leskovec_2016_node2vec.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\MYLYW9DJ\\Grover_Leskovec_2016_node2vec.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\8S27XS9R\\1607.html:text/html}
}

@article{chen_efficient_2020,
	title = {Efficient Construction of Excited-State Hessian Matrices with Machine Learning Accelerated Multilayer Energy-Based Fragment Method},
	volume = {124},
	issn = {1089-5639},
	url = {10.1021/acs.jpca.0c04117},
	doi = {10.1021/acs.jpca.0c04117},
	abstract = {Recently, we have developed a multilayer energy-based fragment ({MLEBF}) method to describe excited states of large systems in which photochemically active and inert regions are separately treated with multiconfigurational and single-reference electronic structure method and their mutual polarization effects are naturally described within the many-body expansion framework. This {MLEBF} method has been demonstrated to provide highly accurate energies and gradients. In this work, we have further derived the {MLEBF} method with which highly accurate excited-state Hessian matrices of large systems are efficiently constructed. Moreover, in combination with recently proposed embedded atom neural network ({EANN}) model we have developed a machine learning ({ML}) accelerated {MLEBF} method (i.e., {ML}-{MLEBF}) in which photochemically inert region is entirely replaced with trained {ML} models. {ML}-{MLEBF} is found to improve computational efficiency of Hessian matrices in particular for large systems. Furthermore, both {MLEBF} and {ML}-{MLEBF} methods are highly parallel and exhibit low-scaling computational cost with multiple {CPUs}. The present developments could motivate combining various {ML} techniques with fragment-based electronic structure methods to explore Hessian-matrix-based excited-state properties of large systems.},
	pages = {5684--5695},
	number = {27},
	journaltitle = {J. Phys. Chem. A},
	author = {Chen, Wen-Kai and Zhang, Yaolong and Jiang, Bin and Fang, Wei-Hai and Cui, Ganglong},
	urldate = {2020-07-09},
	date = {2020-07-09},
	note = {Publisher: American Chemical Society; http://web.archive.org/web/20200709143632/https://pubs.acs.org/doi/10.1021/acs.jpca.0c04117}
}

@article{sanchez-gonzalez_learning_2020-1,
	title = {Learning to Simulate Complex Physics with Graph Networks},
	url = {http://arxiv.org/abs/2002.09405},
	abstract = {Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" ({GNS})---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our {GNS} framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.},
	journaltitle = {{arXiv}:2002.09405 [physics, stat]},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	urldate = {2020-07-09},
	date = {2020-02-21},
	eprinttype = {arxiv},
	eprint = {2002.09405},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics},
	file = {Sanchez-Gonzalez et al_2020_Learning to Simulate Complex Physics with Graph Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\HJ8I3NXV\\Sanchez-Gonzalez et al_2020_Learning to Simulate Complex Physics with Graph Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\NBTAI39T\\2002.html:text/html}
}

@article{duvenaud_convolutional_2015,
	title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
	url = {http://arxiv.org/abs/1509.09292},
	abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	journaltitle = {{arXiv}:1509.09292 [cs, stat]},
	author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
	urldate = {2020-07-10},
	date = {2015-11-03},
	eprinttype = {arxiv},
	eprint = {1509.09292},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Duvenaud et al_2015_Convolutional Networks on Graphs for Learning Molecular Fingerprints.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\RJYA5TGQ\\Duvenaud et al_2015_Convolutional Networks on Graphs for Learning Molecular Fingerprints.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\V9YZTV6C\\1509.html:text/html}
}

@article{atwood_diffusion-convolutional_2016,
	title = {Diffusion-Convolutional Neural Networks},
	url = {http://arxiv.org/abs/1511.02136},
	abstract = {We present diffusion-convolutional neural networks ({DCNNs}), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graph-structured data and used as an effective basis for node classification. {DCNNs} have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on the {GPU}. Through several experiments with real structured datasets, we demonstrate that {DCNNs} are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.},
	journaltitle = {{arXiv}:1511.02136 [cs]},
	author = {Atwood, James and Towsley, Don},
	urldate = {2020-07-10},
	date = {2016-07-08},
	eprinttype = {arxiv},
	eprint = {1511.02136},
	keywords = {Computer Science - Machine Learning},
	file = {Atwood_Towsley_2016_Diffusion-Convolutional Neural Networks.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\84LSVP78\\Atwood_Towsley_2016_Diffusion-Convolutional Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\HBAWXWIG\\1511.html:text/html}
}

@inproceedings{luo_adaptive_2018,
	title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
	url = {https://openreview.net/forum?id=Bkg3g2R9FX},
	abstract = {Adaptive optimization methods such as {AdaGrad}, {RMSprop} and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they...},
	eventtitle = {International Conference on Learning Representations},
	author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
	urldate = {2020-07-11},
	date = {2018-09-27},
	file = {Full Text PDF:C\:\\Users\\mihasik2654\\Zotero\\storage\\HJG8W2P5\\Luo et al. - 2018 - Adaptive Gradient Methods with Dynamic Bound of Le.pdf:application/pdf}
}

@article{luo_adaptive_2019,
	title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},
	url = {http://arxiv.org/abs/1902.09843},
	abstract = {Adaptive optimization methods such as {AdaGrad}, {RMSprop} and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they are observed to generalize poorly compared with {SGD} or even fail to converge due to unstable and extreme learning rates. Recent work has put forward some algorithms such as {AMSGrad} to tackle this issue but they failed to achieve considerable improvement over existing methods. In our paper, we demonstrate that extreme learning rates can lead to poor performance. We provide new variants of Adam and {AMSGrad}, called {AdaBound} and {AMSBound} respectively, which employ dynamic bounds on learning rates to achieve a gradual and smooth transition from adaptive methods to {SGD} and give a theoretical proof of convergence. We further conduct experiments on various popular tasks and models, which is often insufficient in previous work. Experimental results show that new variants can eliminate the generalization gap between adaptive methods and {SGD} and maintain higher learning speed early in training at the same time. Moreover, they can bring significant improvement over their prototypes, especially on complex deep networks. The implementation of the algorithm can be found at https://github.com/Luolc/{AdaBound} .},
	journaltitle = {{arXiv}:1902.09843 [cs, stat]},
	author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
	urldate = {2020-07-11},
	date = {2019-02-26},
	eprinttype = {arxiv},
	eprint = {1902.09843},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Luo et al_2019_Adaptive Gradient Methods with Dynamic Bound of Learning Rate.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\Z4EJJYVC\\Luo et al_2019_Adaptive Gradient Methods with Dynamic Bound of Learning Rate.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\DYCZD58N\\1902.html:text/html}
}

@article{botvinick_deep_2020,
	title = {Deep Reinforcement Learning and its Neuroscientific Implications},
	url = {http://arxiv.org/abs/2007.03750},
	abstract = {The emergence of powerful artificial intelligence is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning, in tasks such as image classification. However, there is another area of recent {AI} work which has so far received less attention from neuroscientists, but which may have profound neuroscientific implications: deep reinforcement learning. Deep {RL} offers a comprehensive framework for studying the interplay among learning, representation and decision-making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep {RL}, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research.},
	journaltitle = {{arXiv}:2007.03750 [cs, q-bio]},
	author = {Botvinick, Matthew and Wang, Jane X. and Dabney, Will and Miller, Kevin J. and Kurth-Nelson, Zeb},
	urldate = {2020-07-12},
	date = {2020-07-07},
	eprinttype = {arxiv},
	eprint = {2007.03750},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	file = {Botvinick et al_2020_Deep Reinforcement Learning and its Neuroscientific Implications.pdf:C\:\\Users\\mihasik2654\\Zotero\\storage\\GIFA926A\\Botvinick et al_2020_Deep Reinforcement Learning and its Neuroscientific Implications.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\mihasik2654\\Zotero\\storage\\EC57F4ZX\\2007.html:text/html}
}